<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/ML/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/ML/" rel="alternate" type="text/html" /><updated>2023-07-06T04:54:34+05:30</updated><id>http://localhost:4000/ML/feed.xml</id><title type="html">ML techniques</title><subtitle>This repo contains the solutions of coding interview questions from various sources.
</subtitle><author><name>Bhoomeendra Singh Sisodiy</name></author><entry><title type="html">Naive Bayes</title><link href="http://localhost:4000/ML/jekyll/2023-07-04-Naive-Bayes.html" rel="alternate" type="text/html" title="Naive Bayes" /><published>2023-07-04T00:00:00+05:30</published><updated>2023-07-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/Naive-Bayes</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-07-04-Naive-Bayes.html"><![CDATA[<p>Given a problem instance to be classified represented by a vector \(x = (x_1, x_2, ..., x_n)\) representing some \(n\) features and a class label \(y\), it assigns to this instance probabilities \(P(y\|x_1, x_2, ..., x_n)\) for each of \(k\) possible outcomes or classes \(y\).</p>

<p>The problem with above formulation is that if the no. of features is large or if a feature can take on a large no. of values then basing such a model on probability tables is infeasible.</p>

<p>To get around this problem, we make the assumption that the value of a particular feature is independent of the value of any other feature, given the class variable. This is called the <strong>naive Bayes assumption</strong>.</p>

\[P(C_k|x_1, x_2, ..., x_n) = \frac{P(C_k)P(x_1, x_2, ..., x_n|C_k)}{P(x_1, x_2, ..., x_n)}\]

<p>For a given sample \(P(x_1, x_2, ..., x_n)\) will remain constant, so we can ignore it. So we have:</p>

\[P(C_k|x_1, x_2, ..., x_n) \propto P(C_k)P(x_1, x_2, ..., x_n|C_k)\]

\[P(C_k|x_1, x_2, ..., x_n) \propto P(C_k)P(x_1|C_k)P(x_2|C_k)...P(x_n|C_k)\]

<p>Now we pick the class \(C_k\) that maximizes the above probability.</p>

<h2 id="text-data">Text Data</h2>

<p>Each word is treated as a feature and its frequency in a given class is used to calculate the probability.</p>

<p>How to handle unseen words? We can use Laplace smoothing.</p>

\[P(w_i|C_k) = \frac{count(w_i, C_k) + \alpha}{count(C_k) + \alpha |V|}\]

<p>Where \(\|V\|\) is the size of the vocabulary and \(\alpha\) is the smoothing parameter. The smoothing parameter is generally set to 1. If \(\alpha\) is infinity then we probability of all the words will be the same. \(\alpha\) acts as a regularizer and prevents overfitting.</p>

<p>Another thing to keep in mind is that the product of probabilities can be very small and can lead to underflow. To avoid this we can take the log of the probabilities and add them.</p>

<h2 id="limitations">Limitations</h2>
<ol>
  <li>The assumption that the features are independent is not true in most cases.</li>
  <li>Class imbalance can lead to poor performance as the model will be biased towards the majority class.</li>
</ol>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Given a problem instance to be classified represented by a vector \(x = (x_1, x_2, ..., x_n)\) representing some \(n\) features and a class label \(y\), it assigns to this instance probabilities \(P(y\|x_1, x_2, ..., x_n)\) for each of \(k\) possible outcomes or classes \(y\).]]></summary></entry><entry><title type="html">Biase Variance Tradeoff</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-Biase_Variance_Tradeoff.html" rel="alternate" type="text/html" title="Biase Variance Tradeoff" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/Biase_Variance_Tradeoff</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-Biase_Variance_Tradeoff.html"><![CDATA[<p><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html">Visit</a></p>

<p>When we talk about the Biase and Variance tradeoff we are talking about the error in the model and these are calculated on multiple(infinte) train and test splits of the data as these measures are statical in nature.</p>

<p><strong>Bias :</strong></p>

<p>Bias is the difference between the average prediction of all the models and the expected Label. The Dataset is also obtained from the a distribution \(P(X,y)\) so for a feature vectors X we a distribution of y which comes from \(P(\frac{Y}{X})\) which means we have to get the expected label from the distribution \(P(\frac{Y}{X})\).</p>

<p>The reason for High bias could be that the model is too simple. Being High bias means that the model is not able to capture the general trend in the data. Low Bias means that the model is able to capture the general trend in the data and is a good situtation to be in.</p>

<p><strong>Variance :</strong></p>

<p>Let’s say we have 50 models and we have to calculate the variance of the models. We calculate the variance of the model by calculating the variability in the output of the models with the formula of variance.</p>

<p>High variance means that the model is not able to generalize well on the data and is overfitting. This mean that if the model is trained on a different the predictions would change drastically. Low variance means that the model is able to generalize well on the data and is a good situation to be in.</p>

<p><strong>Tradeoff :</strong></p>

<p>Pursuing low bias requires increasing complexity which increases variance as now the model has room to overfit the data, and pursuing low variance requires decreasing complexity which increases bias.</p>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Visit]]></summary></entry><entry><title type="html">SVM</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-SVM.html" rel="alternate" type="text/html" title="SVM" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/SVM</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-SVM.html"><![CDATA[<h3 id="svm-for-classification">SVM for Classification</h3>

<p>Let’s say we have +ve and -ve classes what SVM tries to do is find a hyperplane such that it maximizes the margin between the +ve and -ve classes.</p>

<p><strong>Geometric Intuition</strong></p>
<ol>
  <li>Convex hull of the +ve and -ve classes.</li>
  <li>Find the shortest line that connects the convex hulls of the +ve and -ve classes.</li>
  <li>The line that is perpendicular to the shortest line is the separating hyperplane that maximizes the margin between the +ve and -ve classes.</li>
</ol>

<h3 id="formulation">Formulation</h3>
<p>We assume a separating hyperplane of the form \(w^Tx + b = 0\) where \(w\) is the normal vector to the hyperplane and \(b\) is the bias. We assume that the +ve class is on one side of the hyperplane at distance d and the -ve class is on the other side of the hyperplane at a distance c these are the margins and are parallel to the separating hyperplan. We can write the equation of the margins as follows:</p>

\[w^Tx + b = d\]

\[w^Tx + b = -c\]

<p>we can divide the first equation by d and the second equation by c as in doesn’t change the formulation. Now Our goal is to maximize the margin between the margins. Which would be the same as minimizing \(\frac{2}{\left \| w \right \|}\).</p>

<p>Hence the Objective function becomes:</p>

\[w^*,b^* = \underset{w,b}{argmax}\left (\frac{2}{||w||}\right )\]

\[s.t \:\:\: y_i(w^Tx_i + b) \geq 1  \:\:\: \forall i\]

<p>The important part is that the data points which have to be linearly separable for the above formulation to work. But in real world data is not linearly separable so we introduce the concept of slack variables. The slack variables are the distance of the data points from the margins. The objective function becomes:</p>

\[w^*,b^* = \underset{w,b}{argmin}\left (\frac{||w||}{2}\right ) + C \frac{1}{n}\sum_{i}^{n}\varepsilon_i\]

\[s.t \:\:\: y_i(w^Tx_i + b) \geq 1 - \varepsilon_i  \:\:\: \forall i\]

<p>If we look at the formulation we have added a penalty for error in the earlier formulation error was not possible because of the constraints of the formulation which is equivalent to C being infinity. Now we have a tradeoff between the margin and the error. If we increase C we are penalizing the error more and if we decrease C we are penalizing the margin more.</p>

<p>We will convert the above formulation into a Lagrangian formulation and solve it using the Lagrangian multipliers. The Lagrangian formulation is as follows:</p>

\[L(w,b,\varepsilon,\alpha,\mu) = \frac{||w||^{2}}{2} + C \frac{1}{n}\sum_{i}^{n}\varepsilon_i - \sum_{i}^{n}\alpha_i(y_i(w^Tx_i + b) - 1 + \varepsilon_i) - \sum_{i}^{n}\mu_i\varepsilon_i\]

<p>We take the partial derivative of the Lagrangian formulation with respect to w,b and \(\varepsilon\) and equate them to 0. We get the following equations:</p>

\[w = \sum_{i}^{n}\alpha_iy_ix_i\]

\[0 = \sum_{i}^{n}\alpha_iy_i\]

\[\alpha_i = C - \mu_i\]

<p>We substitute the above equations in the Lagrangian formulation and we get the following formulation:</p>

\[L(w,b,\varepsilon,\alpha,\mu) = \sum_{i}^{n}\alpha_i - \frac{1}{2}\sum_{i}^{n}\sum_{j}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j\]

\[s.t \:\:\: \sum_{i}^{n}\alpha_iy_i = 0\]

\[\alpha_i \geq 0\]

\[\mu_i \geq 0\]

\[\varepsilon_i \geq 0\]

\[0 = \sum_{i}^{n}\alpha_iy_i\]

\[0 = \sum_{i}^{n}\mu_i\varepsilon_i\]

<p>The \(L(w,b,\varepsilon,\alpha,\mu)\) looks the same as the formulation for the dual problem of the Hard SVM the only differece is the constrains. The constrains \(\alpha_i \geq 0\)  ,  \(\mu_i \geq 0\) and \(\alpha_i = C - \mu_i\) we combine these constrains and we get \(0 \leq \alpha_i \leq C\).</p>

<h3 id="model-prediction">Model Prediction</h3>

<p>One’s we have identified the support vectors we can calculate the prediction on test as follows:</p>

\[y = sign(\sum_{i}^{n}\alpha_iy_ix_i^Tx + b)\]

<p>As only the support vectors contribute to the prediction we only need to store the support vectors and the corresponding \(\alpha\) values.</p>

<p>The other important thing to note is that the dot product \(x_i^Tx_j\) can be replaced by a kernel function \(K(x_i,x_j)\).</p>

<h3 id="kernel-trick">Kernel Trick</h3>

<p>The kernel trick is a technique in which we can replace the dot product \(x_i^Tx_j\) with a kernel function \(K(x_i,x_j)\) which is a function which takes two vectors as input and outputs a scalar. The kernel function should satisfy the following properties:</p>

\[K(x_i,x_j) = K(x_j,x_i)\]

\[K(x_i,x_j) \geq 0\]

\[\sum_{i}^{n}\sum_{j}^{n}c_ic_jK(x_i,x_j) \geq 0\]

<p>i.e. Symmetric, Positive Semi-definite.</p>

<p>The kernel function can be used to calculate the similarity between two vectors in a higher dimensional space without actually calculating the dot product in the higher dimensional space.</p>

<p><strong>Example:</strong></p>

<h3 id="kernel-functions">Kernel Functions</h3>

<ol>
  <li>Linear Kernel: \(K(x_i,x_j) = x_i^Tx_j\)</li>
  <li>Polynomial Kernel: \(K(x_i,x_j) = (x_i^Tx_j + 1)^d\)</li>
  <li>Gaussian Kernel(rbf kernel): \(K(x_i,x_j) = exp(-\frac{ \left \|  x_i - x_j \right \| ^2}{2\sigma^2})\)</li>
</ol>

<p>Gaussian Kernel is the most commonly used kernel function. The hyperparameter \(\sigma\) is the bandwidth of the kernel function.This is seen as \(\left \|  x_i - x_j \right \| ^2\) is the squared distance between the two vectors. The hyperparameter \(\sigma\) controls the kernel function. If \(\sigma\) is small then the kernel function is very sensitive to the distance between the two vectors (only the vectors which have a distance almost zero will be considered similar) and if \(\sigma\) is large then the kernel function is not very sensitive to the distance between the two vectors. <a href="https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a">Visit</a></p>

<h3 id="svm-for-regression">SVM for Regression</h3>

<p><a href="https://www.niser.ac.in/~smishra/teach/cs460/2020/lectures/lec13_1/">Visit</a></p>

<h3 id="pros">Pros</h3>
<ol>
  <li>SVM is a convex optimization problem hence we are guaranteed to find the global minima.</li>
  <li>SVM is a sparse model as only the support vectors contribute to the prediction.</li>
  <li>SVM can be non-linear using the kernel trick.</li>
  <li>SVM is effective in cases where the number of dimensions is greater than the number of samples.</li>
</ol>

<h3 id="cons">Cons</h3>

<ol>
  <li>SVM is computationally expensive and parallelization is not possible hence not suitable for large datasets.</li>
  <li>SVM is sensitive to outliers.</li>
  <li>SVM is not probabilistic in nature.</li>
</ol>

<h3 id="references">References</h3>
<p><a href="https://www.eurecom.fr/~zuluaga/files/10_soft_svm_notes.pdf">Slides</a></p>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[SVM for Classification]]></summary></entry><entry><title type="html">Bagging</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-Bagging.html" rel="alternate" type="text/html" title="Bagging" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/Bagging</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-Bagging.html"><![CDATA[<p>Given a standard training set \(D\) of size n, bagging generates m new training sets \(D_{i}\), each of size n′, by sampling from \(D\) uniformly and with replacement. By sampling with replacement, some observations may be repeated in each \(D_{i}\).</p>

<p>These are the original, bootstrap, and out-of-bag datasets.</p>

<p>Original dataset \(D\) of size n. Let say we have a bootstrap sample \(D_{i}\) of size n′. Then we would have \(n - n′\) OOB(out of bag) here n′ is the number of unique observations in \(D_{i}\).</p>

<p>Now we train the model on \(D_{i}\) and test it on the OOB dataset. We repeat this process for all the bootstrap samples and then we aggregate the results of all the models to get the final model.</p>

<p>In Bagging we are trying to reduce the variance of the model by training the model on different datasets and aggregating the results. Bagging is also called bootstrap aggregation.</p>

<p>Each learner is high variance, low bias. Aggregating the learners reduces the variance of the ensemble. The reason for variance reduction is that individual learners would have a large variance but when we aggregate the results of all the learners the variance would be reduced. And as the bias of the learners is low the bias of the ensemble would also be low but can be more than the bias of the individual learners.</p>

<p>Pros:</p>
<ol>
  <li>
    <p>Bagging is kind of works as a regularization method as the objective of regularization is to reduce the variance (Prevent Overfitting) of the model.</p>
  </li>
  <li>
    <p>The predictions of the model are more robust (More confident) as we are aggregating the results of multiple models.</p>
  </li>
  <li>
    <p>Bagging can be used for both classification and regression.</p>
  </li>
</ol>

<p>Cons:</p>

<ol>
  <li>
    <p>Bagging take up a lot of memory as we are training multiple models. This can be a problem when we are working with large datasets.</p>
  </li>
  <li>
    <p>Loss of interpretability as we are aggregating the results of multiple models.</p>
  </li>
  <li>
    <p>For weak learner with high bias, bagging will also carry high bias into its aggregated model.</p>
  </li>
</ol>

<h3 id="random-forest">Random Forest</h3>

<h3 id="interview-questions">Interview Questions</h3>

<p>Discuss how bagging handles imbalanced datasets and its impact on the performance of the ensemble?</p>

<p>Bagging can help in cases of data imbalace as we are sampling the data with replacement. This means that the minority class can be sampled more and if needed we can also oversample the minority class. This will help in improving the performance of the model.</p>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Given a standard training set \(D\) of size n, bagging generates m new training sets \(D_{i}\), each of size n′, by sampling from \(D\) uniformly and with replacement. By sampling with replacement, some observations may be repeated in each \(D_{i}\).]]></summary></entry><entry><title type="html">Boosting</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-Boosting.html" rel="alternate" type="text/html" title="Boosting" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/Boosting</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-Boosting.html"><![CDATA[<p>Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. Boosting is One of the methods to do so.</p>

<p>The idea of boosting is to iteratilvely add weak learners each one is trained by taking into account the previous learners’ mistakes. In the end, the weak learners are combined to form a single strong learner.</p>

<p>We can think of boosting using residual error. The idea is to fit a model to the residuals from the previous model. Beaucse the residuals are the errors that the previous model could not fit, a new model that focuses on them will improve the overall fit.</p>

<h3 id="gradient-boosting">Gradient Boosting</h3>

<p>Ref: https://en.wikipedia.org/wiki/Gradient_boosting</p>

<p>Slides : http://www.chengli.io/tutorials/gradient_boosting.pdf</p>

<p>The method which is based of residual error is not flexible enough to be applied to a wide range of loss functions also it is more prone to outliers. Gradient boosting is performing boosting using gradient descent optimization. It is a generalization of boosting to arbitrary differentiable loss functions. It is a greedy algorithm and can overfit if run for too many iterations.</p>

<p>We start with a base learner $F_0(x)$ and then we fit a model to the -ve gradients of the previous model. The model is fit by minimizing the loss function $L(y, F_{m-1}(x))$ wrt $F_{m-1}(x)$. This is done by fitting a weak learner $h_m(x)$ to the gradient $y - F_{m-1}(x)$ (in case of squared loss). The model is then updated by adding the new model $h_m(x)$ to the previous model $F_{m-1}(x)$.</p>

\[F_m(x) = F_{m-1}(x) + \gamma_{m} h_m(x)\]

<p>We also find the optimal step size $\gamma_m$ by minimizing the loss function wrt $\gamma_m$.</p>

\[\gamma_m = argmin_{\gamma} \sum_{i=1}^{N} L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i))\]

<p>We do this for $M$ iterations and then we have the final model $F_M(x)$. The final model is a weighted sum of the weak learners.</p>

\[F_M(x) = \sum_{m=1}^{M} \gamma_m h_m(x)\]

<p>While training the model we can also use a learning rate $\eta$ to control the contribution of each weak learner. The learning rate is used to shrink the contribution of each weak learner. This is done by multiplying the weak learner by the learning rate $\eta$. This help in reducing the overfitting of the model (Regularization).</p>

\[F_m(x) = F_{m-1}(x) + \eta \gamma_{m} h_m(x)\]]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. Boosting is One of the methods to do so.]]></summary></entry><entry><title type="html">DBSCAN</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-DBSCAN.html" rel="alternate" type="text/html" title="DBSCAN" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/DBSCAN</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-DBSCAN.html"><![CDATA[<p>Density-based spatial clustering of applications with noise (DBSCAN) is a clustering algorithm that can handle clusters with different densities. The basic idea is as follows the dense region is the cluster the sparce region is considered noise.  Some of the important definitions are as follows:</p>

<ol>
  <li>
    <p>Core point: A point is a core point if it has at least a minimum number of points (MinPts) within its radius \(\epsilon\).</p>
  </li>
  <li>
    <p>Border point: A point that is not a core point but is within the radius \(\epsilon\) of a core point.</p>
  </li>
  <li>
    <p>Noise point: A point that is neither a core point nor a border point.</p>
  </li>
  <li>
    <p>Density edge: Edge between core points such that the distance between the two core points is less than \(\epsilon\).</p>
  </li>
  <li>
    <p>Density connected: A point \(p\) is density connected to a point \(q\) if there exists a path such that all the edges in the path are density edges.</p>
  </li>
</ol>

<h2 id="algorithm">Algorithm:</h2>

<ol>
  <li>
    <p>Label all points as core, border or noise points and remove noise points.</p>
  </li>
  <li>
    <p>All the core points that are density connected form a cluster.</p>
  </li>
  <li>
    <p>Assign each border point to the closest core point’s cluster.</p>
  </li>
</ol>

<h2 id="important-points">Important Points</h2>
<ol>
  <li>No controle over the number of clusters.</li>
  <li>Can take arbitrary shapes.</li>
  <li>Not sensitive to outliers.</li>
  <li>Varing density clusters can not be handled.</li>
  <li>High dimensional data can not be handled.(Curse of dimensionality)</li>
</ol>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Density-based spatial clustering of applications with noise (DBSCAN) is a clustering algorithm that can handle clusters with different densities. The basic idea is as follows the dense region is the cluster the sparce region is considered noise. Some of the important definitions are as follows:]]></summary></entry><entry><title type="html">Hierarchical Clustring</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-Hierarchical-Clustring.html" rel="alternate" type="text/html" title="Hierarchical Clustring" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/Hierarchical-Clustring</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-Hierarchical-Clustring.html"><![CDATA[<p>Hierarchical Clustering is of two types which are Divisive and Agglomerative.</p>

<p><strong>Agglomerative:</strong> This is a “bottom-up” approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</p>

<p>We start with each observation in its own cluster. Then, we merge the two closest clusters, and repeat until only one cluster remains.</p>

<p>Merging Clusters: How do we define the distance between two clusters?</p>

<ol>
  <li>
    <p>Closest points (Single Link): Compute all pairwise distances between the observations in cluster A and the observations in cluster B, and record the closest distance.</p>
  </li>
  <li>
    <p>Furthest points (Complete Link): Compute all pairwise distances between the observations in cluster A and the observations in cluster B, and record the furthest distance.</p>
  </li>
  <li>
    <p>Average distance (Average Link): Compute all pairwise distances between the observations in cluster A and the observations in cluster B, and record the average distance.</p>
  </li>
  <li>
    <p>Ward’s method: Merge the two clusters such that the increase in the SSE (sum of squared errors) is minimized. This is the same objective function that k-means tries to minimize!(Need to understand this)</p>
  </li>
</ol>

<p><strong>Divisive:</strong> This is a “top-down” approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. We start with all observations in the same cluster and calculate the distance matrix and after that we form a MST (Minimum spanning tree) and then we cut the tree at edges that have the largest distance till no edges are left.</p>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[Hierarchical Clustering is of two types which are Divisive and Agglomerative.]]></summary></entry><entry><title type="html">K-Means</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-KMeans.html" rel="alternate" type="text/html" title="K-Means" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/KMeans</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-KMeans.html"><![CDATA[<p>K means is clustering algorithm where k denotes the number of clusters which we define before hand. The algorithm works as follows:</p>

<ol>
  <li>Randomly initialize k cluster centers (select k points).</li>
  <li>Assign each point to the closest cluster center.</li>
  <li>Recompute the cluster centers by taking the mean of all the points in the new cluster.</li>
  <li>Repeat steps 2 and 3 until convergence (When we don’t see a lot of points change clusters).</li>
</ol>

<p>The above solution is a local optimum and not a global optimum. Mathamatically best cluster centers are the ones that minimize the following objective function:</p>

\[\underset{c}{min} \sum_{i=1}^{k}\sum_{x \in c_i}^{n} \left \| x - \mu_i \right \|^{2}\]

<p>Where \(c\) is the set of clusters, \(c_i\) is the \(i^{th}\) cluster, \(\mu_i\) is the mean of the \(i^{th}\) cluster and \(n\) is the number of points in the \(i^{th}\) cluster. This is a NP hard problem and hence we use the above algorithm to find a local optimum.</p>

<p>Generally we run the above algorithm multiple times with different initializations and choose the one with the lowest objective function value.</p>

<h3 id="important-points">Important Points</h3>
<ol>
  <li>
    <p>K means is sensitive to the initialization of the cluster centers hence we have kmeans++ in which we first pick a centroid and then the points are assigned a probability of begin selected as centroid based on the distance for the current point.</p>
  </li>
  <li>
    <p>K means is sensitive to outliers as the mean is sensitive to outliers to address this we have k medoids in which we use the median instead of the mean.</p>
  </li>
  <li>
    <p>K means is sensitive to the number of clusters we have in the data. To address this we have silhouette score which is a measure of how similar a point is to its own cluster compared to other clusters. The <strong>silhouette score</strong> is calculated as follows: \(s = \frac{b - a}{max(a,b)}\) Where \(a\) is the mean distance between a point and all other points in the same cluster and \(b\) is the mean distance between a point and all other points in the next nearest cluster. The silhouette score is between -1 and 1. A score of 1 means that the point is very similar to its own cluster and very dissimilar to other clusters. A score of 0 means that the point is on the boundary of two clusters. A score of -1 means that the point is assigned to the wrong cluster.</p>
  </li>
  <li>
    <p>If the densities of the clusters are different then k means will not work well. DBSCAN is a clustering algorithm that can handle clusters with different densities.</p>
  </li>
  <li>
    <p>Non-globular clusters are clusters that are not spherical in shape. K means will not work well on non globular clusters.</p>
  </li>
</ol>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[K means is clustering algorithm where k denotes the number of clusters which we define before hand. The algorithm works as follows:]]></summary></entry><entry><title type="html">Linear Discriminant Analysis LDA</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-LDA.html" rel="alternate" type="text/html" title="Linear Discriminant Analysis LDA" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/LDA</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-LDA.html"><![CDATA[]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">PCA</title><link href="http://localhost:4000/ML/jekyll/2023-06-04-PCA.html" rel="alternate" type="text/html" title="PCA" /><published>2023-06-04T00:00:00+05:30</published><updated>2023-06-04T00:00:00+05:30</updated><id>http://localhost:4000/ML/jekyll/PCA</id><content type="html" xml:base="http://localhost:4000/ML/jekyll/2023-06-04-PCA.html"><![CDATA[<p>The idea of PCA is to find new basis in the same space such that the variance of the data is maximized along the new basis. The new basis are called principal components. The first principal component is the direction along which the variance of the data is maximized. The second principal component is the direction orthogonal to the first principal component along which the variance of the data is maximized. The third principal component is the direction orthogonal to the first two principal components along which the variance of the data is maximized. And so on.</p>

<p>Once we have all the basis vectors, we can project the data onto the first few principal components to reduce the dimensionality of the data. The variance captured by the principal components is proportional to the eigenvalues of the corresponding eigenvectors.</p>

<p>We want to find the direction along which the variance of the data is maximized. The variance of the data along a direction is given by the following equation:</p>

\[\underset{u^{T}.u=1}{max} \; Var(u_{1}^T X)\]

<p>Given that \(X\) is mean centered with respect to each feature, we can expand the variance as follows:</p>

\[Var(u_{1}^T X) = (u_{1}^T X)(u_{1}^T X)^{T}\]

\[Var(u_{1}^T X) = u_{1}^{T}Su_{1}\]

<p>Now we want to find a \(u_{1}\) such that the variance is maximized. We can use the Lagrangian formulation to solve this problem. The Lagrangian formulation is as follows:</p>

\[L(u_{1},\lambda) = u_{1}Su_{1} - \lambda(u_{1}^T u_{1} - 1)\]

<p>We take the partial derivative of the Lagrangian formulation with respect to \(u_{1}\) and equate it to 0. We get the following equation:</p>

\[Su_{1} = \lambda u_{1}\]

<p>This is the equation for eigenvalue and eigenvector.</p>

<p>How does variance relate to eigenvalues? We can expand the variance as follows:</p>

\[Var(u_{1}^T X) = u_{1}^{T}Su_{1}\]

\[Var(u_{1}^T X) = u_{1}^{T}(\lambda_{1} u_{1})\]

\[Var(u_{1}^T X) = \lambda_{1} u_{1}^{T}u_{1}\]

\[Var(u_{1}^T X) = \lambda_{1}\]

<h3 id="important-points">Important Points</h3>
<ol>
  <li>
    <p>Because we do eigenvalue decomposition on the covariance matrix, and by defination covariance matrix we have to do mean centering of the data.</p>
  </li>
  <li>
    <p>When we have a large dataset we can use the following trick to do PCA. We can use the following equation to calculate the covariance matrix:\(\frac{1}{n}X^TX\)
as the resulting covariance matrix will be a square matrix of size \(dxd\) where \(d\) is the number of features.</p>
  </li>
  <li>
    <p>Once we have the eigenvectors all the data points can be projected onto the new basis and only the cofficients of the new basis will be stored. This is how we reduce the dimensionality of the data.</p>
  </li>
  <li>
    <p>PCA can be used to remove noise as the components with low variance can be removed.</p>
  </li>
  <li>
    <p>Most variance might not mean the most meaningful for instance lets say we are classifying dog breads then if we remove features with low variance we might remove the features that are important for classification.</p>
  </li>
  <li>
    <p>PCA is a linear transformation and orthogonal hence it can only capture linear relationships.</p>
  </li>
</ol>]]></content><author><name>Bhoomeendra</name></author><category term="Jekyll" /><summary type="html"><![CDATA[The idea of PCA is to find new basis in the same space such that the variance of the data is maximized along the new basis. The new basis are called principal components. The first principal component is the direction along which the variance of the data is maximized. The second principal component is the direction orthogonal to the first principal component along which the variance of the data is maximized. The third principal component is the direction orthogonal to the first two principal components along which the variance of the data is maximized. And so on.]]></summary></entry></feed>