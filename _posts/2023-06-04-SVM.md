---
title: SVM
author: Bhoomeendra 
date: 2023-06-04
category: Jekyll
layout: post
---
### Support Vector Machine for Classification

Let's say we have +ve and -ve classes what SVM tries to do is find a hyperplane such that it maximizes the margin between the +ve and -ve classes. 

__Geometric Intuition__
1. Convex hull of the +ve and -ve classes.
2. Find the shortest line that connects the convex hulls of the +ve and -ve classes.
3. The line that is perpendicular to the shortest line is the separating hyperplane that maximizes the margin between the +ve and -ve classes.

### Formulation
We assume a separating hyperplane of the form $$w^Tx + b = 0$$ where $$w$$ is the normal vector to the hyperplane and $$b$$ is the bias. We assume that the +ve class is on one side of the hyperplane at distance d and the -ve class is on the other side of the hyperplane at a distance c these are the margins and are parallel to the separating hyperplan. We can write the equation of the margins as follows:

$$w^Tx + b = d$$

$$w^Tx + b = -c$$

we can divide the first equation by d and the second equation by c as in doesn't change the formulation. Now Our goal is to maximize the margin between the margins. Which would be the same as minimizing $$\frac{2}{\left \| w \right \|}$$.

Hence the Objective function becomes:

$$ w^*,b^* = \underset{w,b}{argmax}\left (\frac{2}{||w||}\right )$$

$$ s.t \:\:\: y_i(w^Tx_i + b) \geq 1  \:\:\: \forall i$$

The important part is that the data points which have to be linearly separable for the above formulation to work. But in real world data is not linearly separable so we introduce the concept of slack variables. The slack variables are the distance of the data points from the margins. The objective function becomes:

$$ w^*,b^* = \underset{w,b}{argmin}\left (\frac{||w||}{2}\right ) + C \frac{1}{n}\sum_{i}^{n}\varepsilon_i$$

$$ s.t \:\:\: y_i(w^Tx_i + b) \geq 1 - \varepsilon_i  \:\:\: \forall i$$

If we look at the formulation we have added a penalty for error in the earlier formulation error was not possible because of the constraints of the formulation which is equivalent to C being infinity. Now we have a tradeoff between the margin and the error. If we increase C we are penalizing the error more and if we decrease C we are penalizing the margin more.

