---
title: Metrics
author: Bhoomeendra 
date: 2023-05-08
category: Jekyll
layout: post
---

## Classification Metrics
1. __Accuracy:__ 

    This is the most simple measure, defined as the number of accurate predictions divided by the total number of predictions. This will not be a very good metric as if we have imbalanced class labels, then the metric will be biased towards the majority class. Also, when the problem is multi-class, accuracy will not be a good measure as it does not consider the class information.

2. __Confusion Matrix:__

    This is a table that shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data. The matrix is NxN, where N is the number of target values (classes). Performance of such models is commonly evaluated using the data in the matrix. This gives us a complete picture of how well our model works with respect to all the classes.

    __2x2 Confusion Matrix__

    The confusion matrix is for a binary classifier the ground truth are of two types True or False and the prediction can be of two types Positive or Negative. So, the confusion matrix will have 4 values TP, TN, FP, FN.

    TP: True Positive mean that when the ground truth is True and the prediction is positive.

    TN: True Negative mean that when the ground truth is True and the prediction is negative.
    
    FP: False Positive mean that when the ground truth is False and the prediction is positive.
    
    FN: False Negative mean that when the ground truth is False and the prediction is negative.

    |           | Prediction 1 | Prediction 0 | Groud Truth |
    |-----------|--------------|--------------|-------------|
    | Label 1   |       TP     |       TN     | TP+TN       |
    | Label 0   |       FP     |       FN     | FP+FN       |
    | Predicted |      TP+FP   |      TN+FN   |             |

3. __Precision:__
    
    Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answers is of all the positive predictions, how many are actually positive. This metric is useful when the cost of False Positive is high. For example, in the case of death penalty, we would want to be very sure that the person is guilty before giving the death penalty. So, we would want to have a high precision.

    $$Precision = \frac{TP}{(TP+FP)}$$

4. __Recall:__

    Recall is the ratio of correctly predicted positive observations to all observations in the actual class. The question recall answers is: Of all the positive classes, how many did we predict correctly? It is also called Sensitivity. The recall is a good measure to use when the cost of True Negative is high. For example, in the case of a natural calamity, we would want to alert as many people as possible, a wrong alert is not harmful, but a missed alert can be very harmful.

    $$Recall = \frac{TP}{(TP+TN)}$$

5. $$F_\beta$$ Score
    
    The $$F_\beta$$ score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. The $$F_\beta$$ score weights recall more than precision by a factor of $$\beta$$. $$\beta$$ = 1.0 means recall and precision are equally important. for $$\beta < 1.0$$, precision is more important than recall and vice versa.

    $$F_{\beta} =  \frac{1}{ \frac{\beta^2}{1 + \beta^2} . \frac{1}{Recall} +  \frac{1}{1 + \beta^2} . \frac{1}{Precision} }$$
    
    $$F_{\beta} = (1+\beta^2) \frac{Precision*Recall}{(\beta^2*Precision)+Recall}$$

    When $$\beta$$ = 1.0, it is called F1 score, which is the harmonic mean of precision and recall.

    $$F1 = 2 \frac{Precision*Recall}{Precision+Recall}$$

    When $$\beta$$ = 0.5, it is called F0.5 score, which is the weighted harmonic mean of precision and recall.

    $$F0.5 = \frac{1.25*Precision*Recall}{(0.25*Precision)+Recall}$$

    When $$\beta$$ = 2.0, it is called F2 score, which is the weighted harmonic mean of precision and recall.

    $$F2 = \frac{5*Precision*Recall}{(4*Precision)+Recall}$$
    
    __F1 Score Multiclass Classification__

    - __F1 Macro__

        We have to calculate the F1 score for each class and then take the average of all the F1 scores. This is called F1 Macro.

    - __F1 Micro__

        We have to calculate the F1 with the global information of total FP, FN and TP.
            
    - __F1 Weighted__

        We have to calculate the F1 score for each class and then take the weighted average of all the F1 scores. Weights are the number of samples in each class or can be user defined.

6. __AUC ROC Curve:__ _Needs revision_
        
    AUC ROC curve is a performance measurement for binary classification problems. ROC is a curve AUC is the area under the ROC curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for a model which outputs probability. We use a threshold to predict the label, and these labels are used to calculate TPR and FPR. It shows the tradeoff between sensitivity and specificity. The closer the curve follows the upper left-hand border of the ROC space, the better the model (Why). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test. The ROC curve is also called the receiver operating characteristic. The area under the ROC curve (AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative'). 
 

#### Questions 
1. What is the difference between accuracy and F1 score when the classes are balanced?

    Accuracy is the same as F1 macro score when the classes are balanced, but F1 micro will still give a better score as it give equal weightage to all the classes and overall accuracy might look good, but internally, the prediction for some classes might not be good. Both accuracy and F1 macro score will not capture this. 

## Regression Metrics
1. Mean Absolute/Squared Error
3. Root Mean Squared Error # Units
4. R Squared
5. Adjusted R Squared


## Retrieval Metrics
1. MAP
2. NDCG
3. Precision@K
4. Recall@K
5. MRR
6. BPREF
https://www.pinecone.io/learn/offline-evaluation/

## Summarization Metrics
1. ROUGE
2. BLEU
3. METEOR
4. BERTScore
5. Perplexity

## Clustering Metrics
1. Silhouette Score
2. Inter and Intra Cluster Distance ratio

## Segmentation Metrics
1. IoU
2. Precision
3. Recall
4. F1 Score
5. MAP
