---
title: RNN LSTM GRU
author: Bhoomeendra 
date: 2023-07-11
category: Jekyll
layout: post
comments: true
---

Recurrent Neural networks are the solution to the problem of learning sequences data. They are called recurrent because they perform the same task for every element of a sequence but with the added context of the prior inputs. RNN can tackel many problems which map from one to many(Image to text) many to one (Sentence Classification) and many to many (Translation). 
![](https://www.researchgate.net/profile/Bojan_Lukic2/publication/351547862/figure/fig5/AS:1022913150988291@1620892908974/Deep-RNN-left-unrolled-through-time-right-2.ppm)

Let us examin a single timestamp i of the RNN. At i<sup>th</sup> of the RNN we have x<sub>i</sub> input and an input s<sub>i-1</sub> which comes fromt he previous states. we do the following operation $$w.s_{i-1} + U.x_i + b$$ this make the inputs into a single ebedding vector. We apply a sigmoid over this to get s<sub>i</sub> and to get $$\hat{y_i}$$ we do the following $$\hat{y_i} = O(V.S_i + C)$$. $$\hat{y_i}$$ is the output embedding vector which we can mapped to one of the words in the vocubluary.

 



Backpropagation through time

Vanishing and Exploding Gradients

LSTM

GRU

Bidirectional  LSTM


https://www.jefkine.com/general/2018/05/21/2018-05-21-vanishing-and-exploding-gradient-problems/